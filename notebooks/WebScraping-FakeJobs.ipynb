{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0644ae1a-4949-4edd-9ab7-1b01951e620b",
   "metadata": {},
   "source": [
    "### Web Scraping Exercise - Fake Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032ed52-c6e8-46ad-ae11-14503d398901",
   "metadata": {},
   "source": [
    "In this exercise, you'll practice using BeautifulSoup to parse the content of a web page. The page that you'll be scraping, https://realpython.github.io/fake-jobs/, contains job listings. Your job is to extract the data on each job and convert into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1c3e4-bd21-4afc-8846-fa65625a27f8",
   "metadata": {},
   "source": [
    "##### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf8956-0def-46f3-a1d3-0e510099bfb1",
   "metadata": {},
   "source": [
    "Start by performing a GET request on the url above and convert the response into a BeautifulSoup object.  \n",
    "a. Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this title.  \n",
    "b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list.  \n",
    "c. Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end.  \n",
    "d. Take the lists that you have created and combine them into a pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cceecff9-8541-4561-a7c5-42af17f5fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a49916f3-2a61-44a3-abce-d9f3b456cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send get request to the fake jobs URL\n",
    "URL = 'https://realpython.github.io/fake-jobs/'\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8c09c7a8-b703-4727-9e0e-4427f83e6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert response to BeautifulSoup object\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "520a8712-edad-4575-879e-9196634693ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senior Python Developer'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the text from the first job title (navigating to the first card, then finding the text whose tag is h2)\n",
    "soup.find('div', attrs={'class' : 'card'}).h2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cfb94822-f059-450e-96ab-21f5458ad4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through all cards (jobs) on the page\n",
    "jobs = soup.findAll('div', attrs={'class' : 'card'})\n",
    "\n",
    "# Extracting the job title for each job\n",
    "titles_text = [x.h2.text.strip() for x in jobs]\n",
    "\n",
    "# Extracting the companies for each job\n",
    "companies_text = [x.h3.text.strip() for x in jobs]\n",
    "\n",
    "# Extracting the locations for each job\n",
    "locations_text = [x.p.text.strip() for x in jobs]\n",
    "\n",
    "# Extracting the posting date for each job\n",
    "posting_date_text = [x.time.text.strip() for x in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "31f6be09-fac7-4cb6-8959-ec5175f08f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Posting_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Job_Title                     Company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "                Location Posting_Date  \n",
       "0        Stewartbury, AA   2021-04-08  \n",
       "1   Christopherville, AA   2021-04-08  \n",
       "2    Port Ericaburgh, AA   2021-04-08  \n",
       "3      East Seanview, AP   2021-04-08  \n",
       "4    North Jamieview, AP   2021-04-08  \n",
       "..                   ...          ...  \n",
       "95      Lake Abigail, AE   2021-04-08  \n",
       "96        Jacobshire, AP   2021-04-08  \n",
       "97        Port Susan, AE   2021-04-08  \n",
       "98     North Tiffany, AA   2021-04-08  \n",
       "99     Michelleville, AP   2021-04-08  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df = pd.DataFrame({'Job_Title':titles_text, 'Company':companies_text, 'Location':locations_text, 'Posting_Date':posting_date_text})\n",
    "jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00acd572-0186-49cd-8c91-88f1f466afef",
   "metadata": {},
   "source": [
    "##### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af5759-b29c-4513-b0d5-a1a015d1351f",
   "metadata": {},
   "source": [
    "Next, add a column that contains the url for the \"Apply\" button. Try this in two ways.   \n",
    "    a. First, use the BeautifulSoup find_all method to extract the urls.  \n",
    "    b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3fb274e3-c284-49e4-8e6f-1ba6f02eb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column that contains the url for the \"Apply\" button via find_all\n",
    "jobs_df['URL'] = [x.findAll('a')[1].get('href') for x in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f766a567-ce4c-4a96-83ea-428b1e88d33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Posting_Date</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Software Engineer (Python)</td>\n",
       "      <td>Garcia PLC</td>\n",
       "      <td>Ericberg, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/software-engineer-python-10.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Scientist, research (maths)</td>\n",
       "      <td>Manning, Welch and Herring</td>\n",
       "      <td>Laurenland, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/scientist-research-maths-22.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Job_Title                     Company        Location  \\\n",
       "10   Software Engineer (Python)                  Garcia PLC    Ericberg, AE   \n",
       "22  Scientist, research (maths)  Manning, Welch and Herring  Laurenland, AE   \n",
       "\n",
       "   Posting_Date  \\\n",
       "10   2021-04-08   \n",
       "22   2021-04-08   \n",
       "\n",
       "                                                                             URL  \n",
       "10  https://realpython.github.io/fake-jobs/jobs/software-engineer-python-10.html  \n",
       "22  https://realpython.github.io/fake-jobs/jobs/scientist-research-maths-22.html  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "jobs_df[jobs_df['Job_Title'].isin(['Software Engineer (Python)','Scientist, research (maths)'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "069bf196-51a4-4052-bc54-e16e09fc0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column that contains the url for the \"Apply\" button via pattern matching\n",
    "\n",
    "# First, remove all parentheses and commas from job titles\n",
    "titles_stripped = [re.sub(r'[(),]', '', title) for title in titles_text]\n",
    "# Next, replace any slashes with spaces\n",
    "titles_stripped = [re.sub(r'[\\/]', ' ', title) for title in titles_stripped]\n",
    "\n",
    "# Create a list containing the number to append at the end of each URL\n",
    "indices = range(len(titles_stripped))\n",
    "\n",
    "# Define the base URL and extension\n",
    "base_URL = 'https://realpython.github.io/fake-jobs/jobs/'\n",
    "extension = '.html'\n",
    "\n",
    "# Build up the URLs using list comprehension, starting with base URL, adding job title with spaces replaced by hyphens followed by index, and ending with extension\n",
    "built_URLs = [base_URL\n",
    "              + job.lower().replace(' ','-')+'-'+str(index)\n",
    "              + extension \n",
    "              for job, index in zip(titles_stripped,indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "729b5eff-7dcc-459d-adb2-8774199c2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jobs_df['URL'].to_list() == built_URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ba30c-2749-42ae-bfe5-c7081ba52f68",
   "metadata": {},
   "source": [
    "##### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43bbf27-f21e-4f15-af25-1bb955303500",
   "metadata": {},
   "source": [
    "Finally, we want to get the job description text for each job.  \n",
    "    a. Start by looking at the page for the first job, https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job description paragraph.  \n",
    "    b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the description text on that page. For example, if you input \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\".  \n",
    "    c. Use the [.apply method](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) on the url column you created above to retrieve the description text for all of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0cf82c37-bca6-49e2-a848-07c5f36eb415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting job description for first job\n",
    "\n",
    "# Send get request to the job application URL\n",
    "URL = 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# Convert response to BeautifulSoup object\n",
    "soup = BeautifulSoup(response.text)\n",
    "\n",
    "# Extracting the text from the content class\n",
    "soup.find('div', attrs={'class' : 'content'}).p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b4c796a9-0ac5-4d6f-99c5-e3cb3cc7a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function which takes as input a url and returns the description text on that page\n",
    "def get_job_description(job_url):\n",
    "    response = requests.get(job_url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    return soup.find('div', attrs={'class' : 'content'}).p.text\n",
    "\n",
    "get_job_description('https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8f9d7f44-4a33-425c-816b-7032986f7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = jobs_df['URL'].apply(get_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "021e5e6e-f0bd-44c8-9fe2-4e23e241c5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Party prevent live. Quickly candidate change although. Together type music hospital. Every speech support time operation wear often.\n",
       "2                                                                                                                                                                                                                                                                                                     Administration even relate head color. Staff beyond chair recently and off. Own available buy country store build before. Already against which continue. Look road article quickly. International big employee determine positive go Congress. Level others record hospital employee toward like.\n",
       "Name: URL, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b4116-f12d-4784-a565-f92bcea65429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
